# interpretable-ml-risks

Code supporting the 'Interpretative Analysis of Machine Learning Models for Risk Management' FQW performed in HSE Master of Data Science program

### Abstract 
Today, the shift from a purely performance-driven machine learning modelling to the superiority of the performance-explainability trade-off paradigm is undoubtedly visible. In many areas explanations have become vital for machine learning models to gain trust and to be not only successfully deployed, but used in practice. This has given an unprecedented rise to research on diverse black-box explanation methods. Those methods allow to present explanations in many forms, depending on the task, final user, type of model and kind of data, explainability level and form. In this work, an overview of interpretation methods is presented with an aim to examine whether explanations themselves can add value to the business: not only through gaining trust and matching regulations, but through a deliberate real-world action applied to the inputs, that will result in a change of the outcome of the model to a desired one. Financial tasks (closing a deal and bank marketing) are considered as examples. The results reveal a potential of counterfactual explanations to enrich the datasets via actionable interpretations, but the ratio of successes heavily depends on the actorâ€™s ability to intervene, model quality and cost of action. Causal relationships should be carefully taken into consideration during the counterfactual generation process to retain the credibility of the recommendation.